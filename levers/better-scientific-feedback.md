---
title: Better Scientific Feedback
oneliner: 'High-quality reviews before publishing'
image: 'cover-floor.png'
authors:
  - name: Kelvin Yu,
    link: https://twitter.com/kelvinotcelsius
stage: [Course correction]
domain: [Philanthropy, Policy, Academia]
type: [Execution]
---

## Contributors

[Dr. Corin Wagen](https://corinwagen.github.io/public/main/index.html) is a computational/organic chemist who is co-founding a startup in computational chemistry. He recently completed a PhD from Harvard University, where he studied the development and mechanism of selective catalytic reactions. He writes about scientific institutions, progress, and governance [here](https://cwagen.substack.com/). This page is mostly adapted from his post on [why peer review is an imperfect feedback mechanism](https://cwagen.substack.com/p/peer-review-imperfect-feedback).

Thank you to [Dr. Brian Nosek](https://www.cos.io/team/brian-nosek), Founder of the [Center for Open Science](https://www.cos.io/), for his thoughts and feedback on this post.

## Key things to know

- Scientists rarely receive various forms of high-quality criticism of their work, especially before they publish.
- Most feedback comes internally from one’s own research group, which is well-equipped to dispute experimental details or question individual experiments but is much less prone to challenge large-scale scientific assumptions, owing to selection bias. Researchers usually join a group because they agree with one or more key axiomatic assumptions—for instance, a graduate student who joins a lab focusing on using water as a greener solvent for chemical synthesis is unlikely to hear pushback against this point.
- Public feedback is possible, but other students or professors are unlikely to be very critical of a student seminar, with most “feedback” amounting to some form of “Nice Talk.”
- The most reliable form of high-quality external feedback comes from peer review, which only occurs at the end of the scientific process—by the time a manuscript is submitted, it’s often very difficult to incorporate frequently excellent reviewer feedback.
- New ways of giving honest, high-quality feedback on ongoing projects would dramatically improve the quality of science.
- Related: [Structural Diversity](/collection?lever=Structural%2520Diversity)

## Case studies

One of the most successful interventions has been Registered Reports, a publishing format that conducts peer review before data collection, thereby emphasizing the importance of the research question and the quality of methodology. Papers are then provisionally accepted for publication if the authors follow through with the registered methodology.

![Registered Reports pipeline](Better%20Scientific%20Feedback/Untitled.png)
_[From the Center on Open Science](https://www.cos.io/initiatives/registered-reports)_

According to the [Center for Open Science](https://www.cos.io/initiatives/registered-reports), this format rewards adhering to the “hypothetico-deductive model of the scientific method.” It helps eliminate questionable research practices, including low statistical power, selective reporting of results, and publication bias, while allowing flexibility to report serendipitous findings. A couple of papers illustrating its impact can be found here, here, and here.

Another experiment is [Seeds of Science](https://www.theseedsofscience.org/), an organization built around the idea of working papers, which receive community attention from an early stage to help prune and strengthen them. There aren’t obvious bottom-up incentives for adoption by individual scientists, and its use still seems rare (although it’s a young project). This points to the larger challenge that academics are deeply incentivized to publish as much as possible, and ongoing peer review would likely lead to fewer, better papers, which might not benefit researchers in the short-term.

## Further readings

- [Peer Review, Imperfect Feedback | Corin Wagen](https://cwagen.substack.com/p/peer-review-imperfect-feedback?isPin=false)
- [The past, present and future of Registered Reports | Nature](https://www.nature.com/articles/s41562-021-01193-7)
- [Registered Reports | Center for Open Science](https://www.cos.io/initiatives/registered-reports)
- [The limitations to our understanding of peer review | BMC](https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-020-00092-1)
