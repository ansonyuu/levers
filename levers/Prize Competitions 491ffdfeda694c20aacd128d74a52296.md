# Prize Competitions

Stage: Catalytic, Course correction, Growth
Assigned to: Kelvin Yu
Domain: Industry, Philanthropy, Policy
Oneliner: Build stuff to win awards
Outreach owner: Kelvin Yu
Status: Done
Type: Funding, Procurement

## About the co-author

[Kelvin Yu](https://www.kelv.me/) works on AI and R&D policy in Congress. He previously built, invested, and advised technology startups, mostly in the Bay Area. All views presented here are personal and do not represent the views of Congress or the Federal government.

## Key things to know

- Prize competitions are used to incentivize contestants to achieve a specified R&D capability. Prizes can take many forms, including funding (e.g. prize grants), contracts (e.g. guaranteed purchase of the winning solution), or just recognition of the winner (humans are [status-chasing monkeys](https://www.eugenewei.com/blog/2019/2/19/status-as-a-service#:~:text=is%20that%20piece.-,Status%2DSeeking%20Monkeys,-%22It%20is%20a)).
- Prize competitions for contracts are also called ‘challenge-based acquisitions.’ A private sector example by the [Day One Project](https://uploads.dayoneproject.org/2021/09/10150642/Industrial-Policy-Memo.pdf):
    
    > One [Ansari XPrize competition](https://www.xprize.org/) awarded a $10 million prize to a privately funded reliable, reusable spaceship able to carry three people into space (100 km above the Earth’s surface) two times within two weeks. Nearly 30 teams competed in the competition, investing a total of $100 million into finding a solution. The prize competition turned into an acquisition when the winner was licensed by Virgin Galactic—and the solution has become part of the commercial space-flight industry today.
    > 
- Since prize competitions attract a diverse range of participants with various technological approaches, they are an especially useful method for federal agencies to test the operational effectiveness of different technologies. Competitions transfer most of the downside risks to participants, allowing agencies to set ambitious targets without risking great financial loss.
- Although every US federal agency has the authority to support prize competitions up to $50 million, it is not commonly used. [Between 2010 and 2020](https://cset.georgetown.edu/publication/federal-prize-competitions/), agencies hosted 814 competitions through [Challenge.gov](http://challenge.gov/) with combined prizes of $243 million. In comparison, total federal R&D spending totaled $1.3 trillion during the same time—meaning prize pools accounted for 0.02% of R&D spending.
- A Center for Security and Emerging Technology (CSET) study of the 814 federal prize competitions identified three factors that lead some competitions to outperform others in producing positive outcomes. They are:
    
    > First, prizes should reflect the time, effort, and resources involved for participants. Though larger prizes are more likely to attract high quality participants, it is not entirely necessary. 
    
    Second, organizers should design competitions to ensure promising entries have a path towards rapid procurement or scaling versus the traditional acquisition and procurement process. 
    
    Finally, competitions can benefit from access to or the creation of professional networks among participants, which ultimately can improve public-private partnerships.
    > 

## Case studies

******************************************************************Case #1: DARPA’s Grand Challenge******************************************************************

The [DARPA Grand Challenge](https://www.darpa.mil/about-us/timeline/-grand-challenge-for-autonomous-vehicles) was a series of three prize competitions from 2004-2007 that catalyzed the entire field of autonomous vehicles (AVs). In 2004, DARPA utilized its prize competition authority for the first time in the agency’s history, putting on a race with a $1 million prize for whoever could build a self-driving car that drove 142 miles through the Mojave Desert the fastest.

Despite all fifteen finalist vehicles in the first Grand Challenge of 2004 [crashing, failing, or catching fire](https://www.wired.com/story/darpa-grand-challenge-2004-oral-history/), it formally seeded the AV community. Lt. Col. Scott Wadle, DARPA’s liaison to the U.S. Marine Corps, [remarked](https://www.herox.com/blog/159-the-drive-for-autonomous-vehicles-the-darpa-grand#:~:text=As%20Lt.%20Col.%20Scott%20Wadle%2C%20DARPA%E2%80%99s%20liaison%20to%20the%20U.S.%20Marine%20Corps%2C%20said%20of%20the%20people%20involved%3A%20%E2%80%9CThe%20fresh%20thinking%20they%20brought%20was%20the%20spark%20that%20has%20triggered%20major%20advances%20in%20the%20development%20of%20autonomous%20robotic%20ground%20vehicle%20technology%20in%20the%20years%20since.%E2%80%9D): “The fresh thinking they brought was the spark that has triggered major advances in the development of autonomous robotic ground vehicle technology in the years since.”

The second Grand Challenge saw better results—five of 195 teams completed the 132mi race. Stanford’s “Stanley” won first place with a time of 6 hours and 53 minutes, and took home the $2 million prize.

![Untitled](Prize%20Competitions%20491ffdfeda694c20aacd128d74a52296/Untitled.png)

The final event in 2007—the DARPA Urban Challenge—saw a significant elevation in difficulty. Teams were tasked with building an AV capable of handling issues in a mock urban environment like traffic, merging, passing, parking, and negotiating intersections. Cars also had to obey all traffic laws and avoid other robots on the course.

This competition saw DARPA create three prize tiers that it would later use in its other challenges (see Robotics Challenge case study below). First place would receive $2 million (Carnegie Mellon), second place $1 million (Stanford), and 3rd place $500,000 (Virginia Tech). For more information about each competition’s program mechanisms, Wikipedia provides a pretty good [summary](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge).

This trio of challenges was arguably the highest-returning prize competition ever. Not only was immense technical progress made between competitions, but more importantly, they created a community with previously weak ties that could now conduct research or found companies together. Many current AV and deep learning leaders and technologies across defense and commercial applications resulted from the Grand Challenges, [including](https://www.herox.com/blog/159-the-drive-for-autonomous-vehicles-the-darpa-grand#:~:text=ten%20years%20later%2C-,DARPA%20points%20to,-the%20proliferation%20of) those at Waymo, GM, Hyundai, Oshkosh Defense, and TORC Robotics.

![[Where the 2007 DARPA Grand Challenge finalists are today — Michael Dempsey](https://medium.com/@mhdempsey/2007-darpa-grand-challenge-roster-e1d05fccf428)](Prize%20Competitions%20491ffdfeda694c20aacd128d74a52296/Untitled%201.png)

[Where the 2007 DARPA Grand Challenge finalists are today — Michael Dempsey](https://medium.com/@mhdempsey/2007-darpa-grand-challenge-roster-e1d05fccf428)

******************************************Case #2: DARPA’s Robotics Challenge******************************************

The [DARPA Robotics Challenge](https://www.darpa.mil//about-us/timeline/darpa-robotics-challenge) (DRC) was a $3.5 million prize competition that aimed to develop semi-autonomous ground robots [for performing](https://web.archive.org/web/20130120060850/http://www.darpa.mil/Our_Work/TTO/Programs/DARPA_Robotics_Challenge.aspx) "complex tasks in dangerous, degraded, human-engineered environments.” The program was motivated by disasters such as the Fukushima Daiichi hydrogen explosion and the Deepwater Horizon underwater oil spill. It lasted from 2012-2015 and encompassed three competitions: a Virtual Robotics Challenge (VRC) in June 2013 that focused on software; and two live hardware challenges, the DRC Trials in December 2013 and the DRC Finals in June 2015.

In addition to advancing innovation in disaster-response robots, the DRC also [aimed to](https://web.archive.org/web/20130120060850/http://www.darpa.mil/Our_Work/TTO/Programs/DARPA_Robotics_Challenge.aspx#:~:text=will%20play%20a%20catalytic%20role%20in%20development%20of%20robotics%20technology%2C%20allowing%20new%20hardware%20and%20software%20designs%20to%20be%20evaluated%20without%20the%20need%20for%20physical%20prototyping.) “play a catalytic role in the development of robotics technology, allowing new hardware and software designs to be evaluated without the need for physical prototyping.” Therefore, DARPA funded many tools for DRC participants to use, including:

- A robotic hardware platform with arms, legs, torso, and head—enabling teams without hardware expertise or hardware to participate.
- An open-source, real-time, virtual testbed for simulating robots, robot components, and field environments.
- A physical testbed for models that pass the virtual environment.
- Seven Boston Dynamics ATLAS robots and ongoing technical support [were awarded](https://www.darpa.mil/news-events/2013-07-11) to the best-performing VRC teams to continue advancing their software

In the June 2015 finale, DARPA awarded $2 million in the 1st-place prize to a South Korean university team, $1 million in the 2nd-place prize to the Institute for Human & Machine Cognition of Florida team, and $500,000 in the 3rd-place prize to Carnegie Mellon’s team.

Although the competitions did not produce robots that could perform useful real-world work, it nonetheless was a boon to the industry—bringing the world’s top talent together, increasing the accessibility of the field to both hardware and software developers, and most importantly, providing the first large-scale assessment of the field’s progress and gaps. As the CMU team reflected in their follow-up paper *[What Happened at the DARPA Robotics Challenge, and Why?](https://www.cs.cmu.edu/~cga/drc/jfr-what.pdf)* , the DRC “showcased areas where robotics is “solved”, and exposed areas where there is a great deal of work to do.” 

As the paper elaborates, “We have collectively produced many videos showing convincing robot performance. However, in a situation where the researchers did not control the test, most robots, even older and well-tested designs, performed poorly and often fell. It is clear that our field needs to put much more emphasis on building reliable systems, relative to simulations and theoretical results.” These learnings enabled roboticists to hone in on the field’s key challenges, taking us from a world of [A Celebration of Risk (a.k.a., Robots Take a Spill)](https://www.youtube.com/watch?v=7A_QPGcjrh0) and [A Compilation of Robots Falling Down at the DARPA Robotics Challenge](https://www.youtube.com/watch?v=g0TaYhjpOfo) to insane performances like Boston Dynamics’s [Atlas demo](https://www.youtube.com/watch?v=-e1_QhJ1EhQ) in January 2023, less than a decade after the DRC.

## Further readings

- [Federal Prize Competitions | Congressional Research Service (CRS)](https://sgp.fas.org/crs/misc/R45271.pdf)
- [Federal Prize Competitions | Center for Security and Emerging Technology](https://cset.georgetown.edu/publication/federal-prize-competitions/)
- [Prizes and challenge-based acquisition](https://www.mitre.org/sites/default/files/publications/6-prizes-and-challenge-based-acquisition.pdf)
- [2014 Innovative Contracting Case Studies](https://strategicinstitute.org/wp-content/uploads/2016/12/innovative_contracting_case_studies_2014_-_august.pdf)
- [Prize Competitions | Federal Acquisition Institute](https://www.fai.gov/content/prize-competitions)
- [Challenge.gov](Challenge.gov)
- [DARPA Prize Authority 2005 Report to Congress](https://www.grandchallenge.org/grandchallenge/docs/Grand_Challenge_2005_Report_to_Congress.pdf)
- For years (113th and 115th Congress), Bernie Sanders has [proposed](https://archive.is/o/70ivz/https://www.govtrack.us/congress/bills/113/s627/text) creating a $100 billion (0.55% of GDP) pharmaceutical prize pool. The system [would’ve](https://archive.is/70ivz#selection-841.362-845.232) rewarded companies that invented drugs that cured certain conditions, and those drugs would be immediately released without exclusive patent protections.